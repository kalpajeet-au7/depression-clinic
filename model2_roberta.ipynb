{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuOsZlGMGOOb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRtbEeW0HaFN"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60-z2F7kGPHX"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/dep-nlp/\"\n",
        "df_train = pd.read_csv(path+'data/train.csv')\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "possible_labels = df_train.label.unique()\n",
        "\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index\n",
        "print(label_dict)\n",
        "df_train['label'] = df_train.label.replace(label_dict)\n",
        "# dev_clean['label'] = dev_clean.label.replace(label_dict)\n"
      ],
      "metadata": {
        "id": "Ca2FDREil6v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6ZB4JXcHjTc"
      },
      "outputs": [],
      "source": [
        "df_train.label.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrXWyWUF2qCO"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EyeyaO12rns"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[['text', 'label']]\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "tgeCqaVZnM1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV28AR_QTb-Q"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "\n",
        "def preprocess(sentence):\n",
        "  sentence=str(sentence)\n",
        "  sentence=sentence.lower()\n",
        "  cleanr=re.compile('<.*?>')\n",
        "  cleantext=re.sub(cleanr,'',sentence)\n",
        "  rem_url=re.sub(r'http\\S+','',cleantext)\n",
        "  rem_num=re.sub('[0-9]+','',rem_url)\n",
        "  rem_tag=re.sub(r'@\\S+','',rem_num)\n",
        "  tokenizer=RegexpTokenizer(r'\\w+')\n",
        "  tokens=tokenizer.tokenize(rem_tag)\n",
        "  filtered_words=[w for w in tokens if len(w)>2 if not w in stopwords.words('english')]\n",
        "  return \" \".join(filtered_words)\n",
        "\n",
        "df_train['text']=df_train['text'].map(lambda s:preprocess(s))\n",
        "df_train.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULAwe_xt2vqF"
      },
      "outputs": [],
      "source": [
        "model=ClassificationModel('roberta','roberta-base',num_labels=3,use_cuda=True,args={\n",
        "        \"reprocess_input_data\" : True,\n",
        "        \"use_cached_eval_features\":False, \n",
        "        \"overwrite_output_dir\": True, \n",
        "        \"num_train_epochs\": 1}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm_wGcJf24bb"
      },
      "outputs": [],
      "source": [
        "model.train_model(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnwTZZgn3wKa"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNqe424y331Z"
      },
      "outputs": [],
      "source": [
        "df_eval = pd.read_csv(path+'data/dev.csv')\n",
        "df_eval = df_eval[['text', 'label']]\n",
        "df_eval['label'] = df_eval.label.replace(label_dict)\n",
        "print(df_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtWFHzb935_H"
      },
      "outputs": [],
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(df_eval)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cD-rcwvbhDs"
      },
      "outputs": [],
      "source": [
        "print(wrong_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_outputs"
      ],
      "metadata": {
        "id": "uxj79UYlZo1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JD15j8ubYRD"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBj_KuBm5Qwq"
      },
      "outputs": [],
      "source": [
        "df_test= df_eval\n",
        "print(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmtPh-PR5Wwg"
      },
      "outputs": [],
      "source": [
        "predictions, raw_outputs = model.predict(df_test['text'].tolist())\n",
        "print(predictions)\n",
        "print(raw_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore',category=FutureWarning)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df_test.label,predictions))"
      ],
      "metadata": {
        "id": "-AgDCIxBQwnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(df_test.category,predictions))"
      ],
      "metadata": {
        "id": "Al5sTfztzusJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistliBERT test"
      ],
      "metadata": {
        "id": "GeNmsUtaIhNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers &> /dev/null"
      ],
      "metadata": {
        "id": "6VdOC689I-8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm # Progress Bar\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "McCea7KjKk3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "import warnings\n",
        "from transformers import logging as hf_logging\n",
        "hf_logging.set_verbosity_error() # Hidding Huggingface Warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zvhjUIK8Itq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'distilbert-base-cased'"
      ],
      "metadata": {
        "id": "AZEJXwxiI4gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME) # Loading the tokenizer\n"
      ],
      "metadata": {
        "id": "g79Zv2U6IrPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/dep-nlp/\"\n",
        "train = pd.read_csv(path+'clean-data/train-preprocess.csv')\n",
        "dev = pd.read_csv(path+'clean-data/dev-preprocess.csv')"
      ],
      "metadata": {
        "id": "eYmiJoTvJPLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[['text', 'label']]\n",
        "dev = dev[['text', 'label']]"
      ],
      "metadata": {
        "id": "xUBa6dEEJ8ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "possible_labels = train.label.unique()\n",
        "\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index\n",
        "label_dict\n",
        "\n",
        "train['label'] = train.label.replace(label_dict)\n",
        "dev['label'] = dev.label.replace(label_dict)"
      ],
      "metadata": {
        "id": "9utSd1LIKT_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "RJ7KMmnXKar3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 250\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME,  \n",
        "                                                add_special_tokens=True,\n",
        "                                                max_length=MAX_LENGTH, \n",
        "                                                pad_to_max_length=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer.encode_plus(sentence, \n",
        "                                       add_special_tokens=True, \n",
        "                                       max_length=MAX_LENGTH, \n",
        "                                       pad_to_max_length=True, \n",
        "                                       return_attention_mask=True, \n",
        "                                       return_token_type_ids=True, \n",
        "                                       truncation=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')"
      ],
      "metadata": {
        "id": "VszBCmz_Il7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = DistilBertConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, output_attentions=True)\n",
        "DistilBERT = TFDistilBertModel.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='masked_token', dtype='int32') \n",
        "\n",
        "embedding_layer = DistilBERT(input_ids = input_ids_in, attention_mask = input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(3, activation='softmax')(X)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model.layers[:3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sP7fmvzyKpav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train['text']\n",
        "x_val = dev['text']"
      ],
      "metadata": {
        "id": "sSdQzunlLSfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tokenize(x_train, tokenizer)\n",
        "X_val = tokenize(x_val, tokenizer)"
      ],
      "metadata": {
        "id": "mnRHHOvTLSaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['label'].to_list()\n",
        "y_val = dev['label'].to_list()"
      ],
      "metadata": {
        "id": "jmYNRReeLmzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_checkpoint = ModelCheckpoint(filepath=output_dir+'/weights.{epoch:02d}.hdf5', save_weights_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(patience=3, # Stop after 3 epochs of no improvement\n",
        "                               monitor='val_loss', # Look at validation_loss\n",
        "                               min_delta=0, # After 0 change\n",
        "                               mode='min', # Stop when quantity has stopped decreasing\n",
        "                               restore_best_weights=False, # Don't Restore the best weights\n",
        "                               verbose=1) \n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', # Look at validation loss\n",
        "                              min_lr=0.000001, # Lower bound of learning rate\n",
        "                              patience=1, # Reduce after 1 with little change\n",
        "                              mode='min', # Stop when quantity has stopped decreasing\n",
        "                              factor=0.1, # Reduce by a factor of 1/10\n",
        "                              min_delta=0.01, # Minimumn change needed\n",
        "                              verbose=1)"
      ],
      "metadata": {
        "id": "1QzsnXmiKuim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val"
      ],
      "metadata": {
        "id": "u52qhQZ5MgQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val"
      ],
      "metadata": {
        "id": "gT_V8H-eMiVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, \n",
        "                    y_train, \n",
        "                    epochs = 10,\n",
        "                    batch_size=16, \n",
        "                    validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "cFXCiY_qK6w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU tests"
      ],
      "metadata": {
        "id": "RRDo9jm-IkG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "id": "R8IUEHVk1PT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import *\n",
        "virtual_memory()"
      ],
      "metadata": {
        "id": "ApKgu-ZY1ahb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU count and name\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "mxNGxxT_2FAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use this command to see GPU activity while doing Deep Learning tasks, for this command 'nvidia-smi' and for above one to work, go to 'Runtime > change runtime type > Hardware Accelerator > GPU'\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "CUk-cv4i2Ggc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "metadata": {
        "id": "Twj_iiiK2ZGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBkyOb4O4j7r"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore',category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(df_test.label,predictions))"
      ],
      "metadata": {
        "id": "pDUoDHOGd9Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tes = pd.read_csv('test_data.tsv', sep='\\t')\n",
        "df_tes['category'] = predictions\n",
        "df_tes.to_csv('scube_run2.csv')"
      ],
      "metadata": {
        "id": "frx41kTYaMLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "U8w3Yg_gars2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tes = pd.read_csv('roberta.csv')\n",
        "df_tes.to_csv('roberta.tsv', sep='\\t',index=False)\n",
        "df_tes.head(20)"
      ],
      "metadata": {
        "id": "Q4Gc9ybhcUXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tes['category'].value_counts()"
      ],
      "metadata": {
        "id": "VbtBqVS2cUpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(8,8)})\n",
        "#sns.countplot(trainDF_Sheet_1['category'])\n",
        "sns.countplot(df_test['category'])"
      ],
      "metadata": {
        "id": "x8y3GNvDcU1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "G_JD0FjcaqEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5LbvCHWla34A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tes = pd.read_csv('roberta.csv')\n",
        "df_tes.to_csv('scube_run2.tsv', sep='\\t',index=False)\n",
        "df_tes.head(20)"
      ],
      "metadata": {
        "id": "9I9IxRVOagJz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RRDo9jm-IkG5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}